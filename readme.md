
# Code and Data for Paper: "New Paradigm for Evaluating Scholar Summaries: A Facet-aware Metric & Meta-evaluation Benchmark"

This repository contains the datasets, code, and instructions supporting the paper *"New Paradigm for Evaluating Scholar Summaries: A Facet-aware Metric & Meta-evaluation Benchmark."* Our research introduces a novel Facet-aware Metric (FM metric) and an extensive meta-evaluation benchmark to assess the quality of text summarization in various domains.

---

## **Repository Structure**
This repository is organized as follows:

```
.
├── dataset/                  # Processed datasets for our research
│   ├── pubmed.json           # Summarization dataset for medical articles
│   └── arxiv.json            # Summarization dataset for academic papers
│
├── evaluation_code/          # Evaluation scripts for metric and model-level analysis
│   ├── metric_level_arxiv.py
│   ├── metric_level_pubmed.py
│   ├── model_level_arxiv.py
│   ├── model_level_pubmed.py
│   └── eg. metric_level_pubmed_result.pdf  # Example evaluation result
│
├── FM metric code/           # Implementation of our Facet-aware Metric system
│   ├── 1aspect_extraction.py # Extracts key evaluation facets from summaries
│   └── 2evaluation.py        # Evaluates summaries using the extracted facets
│
├── Contact Form.pdf          # Form for reporting issues or requesting content removal
├── appendix.pdf              # Supplementary materials including tables and discussions
└── README.md                 # Repository overview and instructions
```

---

## **Datasets**

We provide two key datasets in this repository:
- **`pubmed.json`**: Contains medical articles and associated summaries.
- **`arxiv.json`**: Contains summaries for academic papers in areas such as economics and computer science.

### **Data Structure**
Each dataset entry is structured as follows:
- **`article`**: Original article text.
- **`human`**: Human-written reference summary.
- **Model-generated summaries**: Summaries generated by models like GPT-3.5, LLaMA2, BigBird-Pegasus, etc.
- **Evaluation metrics**: Multiple metrics (FM metric, BERTScore, ROUGE, QuestEval) are included for assessment.

---

## **Evaluation Instructions**

We provide a two-step process to perform the evaluation using our Facet-aware Metric (FM metric).

### **Step 1: Extract Evaluation Facets**
Run the following script to extract evaluation facets:
```bash
python FM\ metric\ code/1aspect_extraction.py
```

This script processes the input summaries and extracts key aspects (e.g., factuality, coherence) for evaluation.

### **Step 2: Evaluate Summaries**
After extracting the facets, evaluate the summaries using:
```bash
python FM\ metric\ code/2evaluation.py
```

This will compute the FM metric scores for all summaries in the dataset.

### **Model-Level Evaluation**
To evaluate the performance of individual models, run:
```bash
python evaluation_code/model_level_pubmed.py
python evaluation_code/model_level_arxiv.py
```

### **Metric-Level Evaluation**
To compare the FM metric against other metrics (e.g., ROUGE, BERTScore), use:
```bash
python evaluation_code/metric_level_pubmed.py
python evaluation_code/metric_level_arxiv.py
```

**Note:** Example output can be found in `eg. metric_level_pubmed_result.pdf`.

---

## **Sample Command**
```bash
# Evaluate summaries on the PubMed dataset using FM metric
python FM\ metric\ code/1aspect_extraction.py --dataset dataset/pubmed.json
python FM\ metric\ code/2evaluation.py --input extracted_aspects.json
```

---

## **Additional Resources**
- **Contact Form**: For reporting dataset or content issues, please refer to `Contact Form.pdf`.
- **Appendix**: For supplementary discussions and tables supporting our research, refer to `appendix.pdf`.